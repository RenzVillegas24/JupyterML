{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Dimensionality Reduction (Without Categorical Data in Features)\n",
    "### Dr. Robert G. de Luna, PECE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "\n",
    "**Feature Selection** is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\n",
    "\n",
    "Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three benefits of performing Feature Selection before modeling your data are:\n",
    "\n",
    "**1. Reduces Overfitting:** Less redundant data means less opportunity to make decisions based on noise.\n",
    "\n",
    "**2. Improves Accuracy:** Less misleading data means modeling accuracy improves.\n",
    "\n",
    "**3. Reduces Training Time:** Less data means that algorithms train faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CHECK THE VERSION OF LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "# numpy\n",
    "import numpy\n",
    "print('numpy: {}'.format(numpy.__version__))\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "# pandas\n",
    "import pandas\n",
    "print('pandas: {}'.format(pandas.__version__))\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import pandas as pd\n",
    "\n",
    "# To allow plots to appear within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO LOAD THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.read_csv('diabetes.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DETERMINE THE DIMENSIONS OF THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO PEEK AT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO SEE THE STATISTICAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO SEE THE CLASS DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.groupby('Outcome').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO SHOW THE UNIVARIATE PLOT (BOX and WHISKER PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO SHOW THE HISTOGRAM FOR THE DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR THE MULTIVARIATE PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Scatter Plot Matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(dataset)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CREATE THE MATRIX OF INDEPENDENT VARIABLE, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 0:8].values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CREATE THE MATRIX OF DEPENDENT VARIABLE, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset.iloc[:,8].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Create Machine Learning Models with K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. USING LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Import the Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# To Instantiate the Model (Using the Default Parameters)\n",
    "logistic_regression = LogisticRegression(max_iter=100000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Apply K-fold Cross Validation for the Logistic Regression Model Performance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "k_Fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=logistic_regression, X=X, y=Y, cv=k_Fold, scoring='accuracy')\n",
    "accuracies_average = accuracies.mean()\n",
    "accuracies_deviation = accuracies.std()\n",
    "print(\"ACCURACIES IN K-FOLDS:\")\n",
    "print(accuracies)\n",
    "print('')\n",
    "print(\"AVERAGE ACCURACY OF K-FOLDS:\")\n",
    "print(accuracies_average)\n",
    "print('')\n",
    "print(\"ACCURACY DEVIATION OF K-FOLDS:\")\n",
    "print(accuracies_deviation)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. USING K NEAREST NEIGHBORS WITH K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Import the K Nearest Neighbors Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# To Instantiate the Model (Using the Default Parameters)\n",
    "k_nearest_neighbors = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Apply K-fold Cross Validation for the K Nearest Neighbors Model Performance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "k_Fold = StratifiedKFold (n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=k_nearest_neighbors, X=X, y=Y, cv=k_Fold, scoring='accuracy')\n",
    "accuracies_average = accuracies.mean()\n",
    "accuracies_deviation = accuracies.std()\n",
    "print(\"ACCURACIES IN K-FOLDS:\")\n",
    "print(accuracies)\n",
    "print('')\n",
    "print(\"AVERAGE ACCURACY OF K-FOLDS:\")\n",
    "print(accuracies_average)\n",
    "print('')\n",
    "print(\"ACCURACY DEVIATION OF K-FOLDS:\")\n",
    "print(accuracies_deviation)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. USING SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Import the Support Vector Machine Model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# To Instantiate the Model (Using Majority of Default Parameters)\n",
    "support_vector_machine = SVC(kernel = 'rbf', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Apply K-fold Cross Validation for the Support Vector Machine Model Performance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "k_Fold = StratifiedKFold (n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=support_vector_machine, X=X, y=Y, cv=k_Fold, scoring='accuracy')\n",
    "accuracies_average = accuracies.mean()\n",
    "accuracies_deviation = accuracies.std()\n",
    "print(\"ACCURACIES IN K-FOLDS:\")\n",
    "print(accuracies)\n",
    "print('')\n",
    "print(\"AVERAGE ACCURACY OF K-FOLDS:\")\n",
    "print(accuracies_average)\n",
    "print('')\n",
    "print(\"ACCURACY DEVIATION OF K-FOLDS:\")\n",
    "print(accuracies_deviation)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Perform Different Feature Selection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Univariate Selection\n",
    "\n",
    "Statistical tests can be used to select those features that have the strongest relationship with the output variable.\n",
    "\n",
    "The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "The example below uses the Chi-squared (chi^2) statistical test for non-negative features to select 4 of the best features from the Pima Indians onset of diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Import the Class of SelectKBest and chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# For the List of Features\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "print('List of Features:')\n",
    "print(features)\n",
    "print('')\n",
    "\n",
    "# To Perform Feature Selection with SelectKBest\n",
    "selection_method_skb = SelectKBest(score_func=chi2, k=4)\n",
    "selection_fit_skb = selection_method_skb.fit(X, Y)\n",
    "\n",
    "# To Show the Results of Feature Selection\n",
    "selection_scores = selection_fit_skb.scores_\n",
    "print('Selection Scores: %s' % selection_scores)\n",
    "print('')\n",
    "\n",
    "print('Features and the Selection Scores')\n",
    "list(zip(features, selection_scores))\n",
    "\n",
    "#Print('Summary of the Selected Features')\n",
    "#selected_features_skb = selection_fit_skb.transform(X)\n",
    "#print(selected_features_skb)\n",
    "#print('')\n",
    "\n",
    "feat_importances = pd.Series(selection_scores, index=features)\n",
    "feat_importances = feat_importances.nlargest(8)\n",
    "feat_importances.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores):\n",
    "   \n",
    "Insulin, Glucose, Age, and BMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Recursive Feature Elimination\n",
    "\n",
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n",
    "\n",
    "It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "The example below uses RFE with the logistic regression algorithm to select the top 4\n",
    "features. The choice of algorithm does not matter too much as long as it is skillful and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Import the Class of RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# For the List of Features\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "print('List of Features:')\n",
    "print(features)\n",
    "print('')\n",
    "\n",
    "# To Perform Feature Extraction with RFE using Logistic Regression as the Model\n",
    "selection_method_rfe = RFE(estimator=logistic_regression, n_features_to_select=4) \n",
    "# Note: Logistic Regression has \"coef_\" or \"feature_importances_\" attributes (unlike KNN and SVM) that can be used by RFE \n",
    "selection_fit_rfe = selection_method_rfe.fit(X, Y)\n",
    "\n",
    "# To Show the Results of Feature Selection\n",
    "number_features = selection_fit_rfe.n_features_\n",
    "selected_features = selection_fit_rfe.support_\n",
    "features_ranking = selection_fit_rfe.ranking_\n",
    "\n",
    "print(\"Number of Features: %s\" % number_features)\n",
    "print(\"Selected Features: %s\" % selected_features)\n",
    "print(\"Feature's Ranking: %s\" % features_ranking)\n",
    "print('')\n",
    "\n",
    "print('Features, Selected Features, and the Ranking Score:')\n",
    "list(zip(features, selected_features, features_ranking))\n",
    "\n",
    "feat_importances = pd.Series(features_ranking, index=features)\n",
    "feat_importances = feat_importances.nlargest(8)\n",
    "feat_importances.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You can see that using the logistic regression model as estimator, the 4 attributes chosen are:\n",
    "\n",
    "Pregnancies, Glucose, BMI, and DiabetesPedigreeFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.\n",
    "\n",
    "Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.\n",
    "\n",
    "In the example below, we use PCA and select 4 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Import the Class of PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For the List of Features\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "print('List of Features:')\n",
    "print(features)\n",
    "print('')\n",
    "\n",
    "# To Perform Feature Selection with PCA\n",
    "selection_method_pca = PCA(n_components=4)\n",
    "selection_fit_pca = selection_method_pca.fit(X)\n",
    "\n",
    "# To Summarize the Principal Components\n",
    "explained_variance = selection_fit_pca.explained_variance_ratio_\n",
    "print(\"Explained Variance: %s\" % explained_variance)\n",
    "print('')\n",
    "\n",
    "print(\"For the Transformed Component:\")\n",
    "components = selection_fit_pca.components_\n",
    "print(components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the transformed dataset (4 principal components) bare little resemblance to the source data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Feature Importance\n",
    "\n",
    "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\n",
    "\n",
    "In the example below we construct a ExtraTreesClassifier classifier for the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Import Class ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# For the List of Features\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "print('List of Features:')\n",
    "print(features)\n",
    "print('')\n",
    "\n",
    "# To Perform Feature Importance with Extra Trees Classifier\n",
    "model = ExtraTreesClassifier(random_state=0)\n",
    "model.fit(X, Y)\n",
    "\n",
    "# To Show the Results of Feature Importance\n",
    "importance = model.feature_importances_\n",
    "print('Importance Score: %s' % importance)\n",
    "print('')\n",
    "\n",
    "print('Features and the Importance Score:')\n",
    "list(zip(features, importance))\n",
    "\n",
    "feat_importances = pd.Series(importance, index=features)\n",
    "feat_importances = feat_importances.nlargest(20)\n",
    "feat_importances.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we are given an importance score for each attribute where the larger scores are the more important attributes. The scores suggest the importance of:\n",
    "\n",
    "Glucose, Age, BMI, and DiabetesPedigreeFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix with Heatmap is Applicable Only to determine Corellation Between Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation states how the features are related to each other or the target variable.\n",
    "Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n",
    "Heatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the List of Features\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "print('List of Features:')\n",
    "print(features)\n",
    "print('')\n",
    "\n",
    "# To Compute for the Correlation of each Features in the Dataset\n",
    "correlation_matrix = dataset.corr()\n",
    "correlation_features = correlation_matrix.index\n",
    "plot.figure(figsize=(20,20))\n",
    "\n",
    "# To Plot the Heatmap\n",
    "g=sns.heatmap(dataset[correlation_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all features have low correlation with the outcome. That is, correlation matrix is not applicable for the classification task since output in classification are discreet values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dr. Robert G. de Luna, PECE\n",
    "rgdeluna@pup.edu.ph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
